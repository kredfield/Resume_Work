{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import itertools\n",
    "import category_encoders as ce\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\kredfield\\Documents\\Berkeley\\W207\\FinalProject\\Inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "\n",
    "First, we import the relevant datasets. These datasets were downloaded as a zip from https://www.kaggle.com/c/9120/download-all. You can download the data and assign your os.chdir to the download location to run this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau = pd.read_csv(\"./bureau.csv\")\n",
    "bureau_balance = pd.read_csv(\"./bureau_balance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-21 08:19:14.581880\n",
      "complete at 2018-08-21 08:19:47.722676\n",
      "total runtime: 0:00:33.140796\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "train_raw = pd.read_csv(\"./application_train.csv\")\n",
    "dev_data = pd.read_csv(\"./application_test.csv\")\n",
    "credit_card_balance = pd.read_csv(\"./credit_card_balance.csv\")\n",
    "cash_balance = pd.read_csv(\"./POS_CASH_balance.csv\")\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230633, 121)\n",
      "(76878, 121)\n",
      "(230633,)\n",
      "(76878,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_raw[\"TARGET\"]\n",
    "train = train_raw.drop(labels=\"TARGET\", axis=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train,train_labels)\n",
    "for df in [train_data, test_data, train_labels, test_labels]:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-20 16:12:09.337189\n",
      "complete at 2018-08-20 16:12:09.581857\n",
      "total runtime: 0:00:00.244668\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "#create mini sets just for debugging\n",
    "mini_train_data   = train_data[:10000]\n",
    "mini_train_labels = train_labels[:10000]\n",
    "mini_test_data    = test_data[:10000]\n",
    "mini_test_labels  = test_labels[:10000]\n",
    "\n",
    "\n",
    "#make mini versions of bureau and bureau balance for debugging\n",
    "mini_bureau         = bureau[:100000]\n",
    "mini_bureau_balance = bureau_balance[:100000]\n",
    "mini_cash           = cash_balance[:100000]\n",
    "mini_credit         = credit_card_balance[:100000]\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions\n",
    "\n",
    "The data requires a few main cleaning functions: \n",
    "    1. categorizing string variables\n",
    "    2. replacing infinitea and na variables\n",
    "    3. scaling the data to 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def handle_missing_inf(df,only_num = False):\n",
    "    if only_num in [True,False]:\n",
    "        print(\"Entered original loop for replace_na\")\n",
    "        mini_df = df[:10]\n",
    "        g = mini_df.columns.to_series().groupby(mini_df.dtypes).groups\n",
    "        type_dict = {k.name: v for k, v in g.items()}\n",
    "        num_cols = []\n",
    "        for x in mini_df.columns:        \n",
    "            try:\n",
    "                if x in type_dict['float64'] or x in type_dict['int64']:\n",
    "                    num_cols.append(x)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for x in num_cols:\n",
    "            df[x] = np.where(np.isinf(df[x]),np.nan,df[x])\n",
    "            df[x] = df[x].fillna(df[x].mean())\n",
    "            \n",
    "        if only_num is True:\n",
    "            print(\"removing all non-numeric columns\")\n",
    "            return df[num_cols]\n",
    "        \n",
    "        else:\n",
    "            return df\n",
    "    else:\n",
    "        print('only_num parameter may only be True or False')\n",
    "        raise\n",
    "\n",
    "#deprecated\n",
    "def replace_inf(df):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        for clm in df.columns:\n",
    "            if not df[clm].dtype == \"O\":\n",
    "                df[clm] = np.where(np.isinf(df[clm]),np.nan,df[clm])\n",
    "    return df\n",
    "\n",
    "def merge_bureau(df):\n",
    "    df = pd.merge(df,bureau,on=\"SK_ID_CURR\", how='left', indicator=True)\n",
    "    df = df[df[\"_merge\"] != \"right_only\"]\n",
    "    df[\"credit_history\"] = np.where(df._merge == \"left_only\",0,1)\n",
    "    del df[\"_merge\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_string_vars(df):\n",
    "    str_cols = []\n",
    "    mini_df = df[:10]\n",
    "#     for clm in mini_df.columns:\n",
    "#             if mini_df[clm].dtype == 'O':\n",
    "#                 if mini_df[clm].nunique() < 60:\n",
    "#                     str_cols.append(clm)\n",
    "    encoder = ce.OneHotEncoder()\n",
    "    df = encoder.fit_transform(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Confusion Matrix Plotting Function\n",
    "\n",
    "We'll need to plot a confusion matrix for later classification functions. We'll define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now, we're going to create some additional features out of the observation's credit histories. Importantly, these features map well to the features used in constructing the FICO credit score. First, we look at their history in repaying their loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-20 16:12:09.712192\n",
      "Entered original loop for replace_na\n",
      "removing all non-numeric columns\n",
      "complete at 2018-08-20 16:24:08.866850\n",
      "total runtime: 0:11:59.154658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715448</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-11</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-12</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-13</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-20</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-21</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-22</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-23</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-24</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-25</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-26</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5715449</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5715449</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5715449</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-11</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-12</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-13</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-14</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-15</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-16</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-17</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-18</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-19</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-20</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-21</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-22</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-23</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-24</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-25</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-26</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-27</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-28</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-29</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-30</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-31</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-32</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-33</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-34</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-35</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5720034</td>\n",
       "      <td>-36</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SK_ID_BUREAU  MONTHS_BALANCE STATUS\n",
       "0          5715448               0      C\n",
       "1          5715448              -1      C\n",
       "2          5715448              -2      C\n",
       "3          5715448              -3      C\n",
       "4          5715448              -4      C\n",
       "5          5715448              -5      C\n",
       "6          5715448              -6      C\n",
       "7          5715448              -7      C\n",
       "8          5715448              -8      C\n",
       "9          5715448              -9      0\n",
       "10         5715448             -10      0\n",
       "11         5715448             -11      X\n",
       "12         5715448             -12      X\n",
       "13         5715448             -13      X\n",
       "14         5715448             -14      0\n",
       "15         5715448             -15      0\n",
       "16         5715448             -16      0\n",
       "17         5715448             -17      0\n",
       "18         5715448             -18      0\n",
       "19         5715448             -19      0\n",
       "20         5715448             -20      X\n",
       "21         5715448             -21      X\n",
       "22         5715448             -22      X\n",
       "23         5715448             -23      X\n",
       "24         5715448             -24      X\n",
       "25         5715448             -25      X\n",
       "26         5715448             -26      X\n",
       "27         5715449               0      C\n",
       "28         5715449              -1      C\n",
       "29         5715449              -2      C\n",
       "...            ...             ...    ...\n",
       "9970       5720034              -7      C\n",
       "9971       5720034              -8      C\n",
       "9972       5720034              -9      C\n",
       "9973       5720034             -10      C\n",
       "9974       5720034             -11      C\n",
       "9975       5720034             -12      C\n",
       "9976       5720034             -13      C\n",
       "9977       5720034             -14      C\n",
       "9978       5720034             -15      C\n",
       "9979       5720034             -16      C\n",
       "9980       5720034             -17      C\n",
       "9981       5720034             -18      C\n",
       "9982       5720034             -19      C\n",
       "9983       5720034             -20      C\n",
       "9984       5720034             -21      C\n",
       "9985       5720034             -22      C\n",
       "9986       5720034             -23      C\n",
       "9987       5720034             -24      C\n",
       "9988       5720034             -25      C\n",
       "9989       5720034             -26      C\n",
       "9990       5720034             -27      C\n",
       "9991       5720034             -28      C\n",
       "9992       5720034             -29      C\n",
       "9993       5720034             -30      C\n",
       "9994       5720034             -31      C\n",
       "9995       5720034             -32      C\n",
       "9996       5720034             -33      C\n",
       "9997       5720034             -34      C\n",
       "9998       5720034             -35      C\n",
       "9999       5720034             -36      C\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "mini_bureau_balance = bureau_balance[:10000]\n",
    "mini_bureau = bureau[:10000]\n",
    "\n",
    "\n",
    "#replace string vars with negative numbers, we're just trying to pull out the dpd loans\n",
    "bureau_balance[\"STATUS_encode\"] = bureau_balance.STATUS.replace(\"C\",-1)\n",
    "bureau_balance[\"STATUS_encode\"] = bureau_balance.STATUS_encode.replace(\"X\",0)\n",
    "\n",
    "#then cast that new column as numeric\n",
    "bureau_balance[\"STATUS_encode\"] = bureau_balance[\"STATUS_encode\"].astype('int64')\n",
    "\n",
    "#conver months_balance, which is negative, to months_ago, which makes more intuitive sense\n",
    "bureau_balance[\"months_ago\"] = bureau_balance[\"MONTHS_BALANCE\"] *-1\n",
    "\n",
    "#perform operations by group since the data is long\n",
    "grp = bureau_balance.groupby(\"SK_ID_BUREAU\")\n",
    "\n",
    "#get the max number of months of credit for this loan\n",
    "max_credit_months = grp.apply(lambda x: np.amax(x[\"months_ago\"]))\n",
    "max_credit_months.name = \"max_credit_months\"\n",
    "\n",
    "#get the max value for dpd, which maps to how late the person ever was. 5 is the worst, 0 is the best\n",
    "max_dpd = grp.apply(lambda x: np.amax(x[\"STATUS_encode\"]))\n",
    "max_dpd.name = \"max_dpd\"\n",
    "\n",
    "#get the number of times the person was ever dpd\n",
    "ever_dpd_count = grp.apply(lambda x: x[x[\"STATUS_encode\"]>0][\"SK_ID_BUREAU\"].count())\n",
    "ever_dpd_count.name = \"ever_dpd_count\"\n",
    "\n",
    "#and then merge each one back onto the original file\n",
    "\n",
    "for feature in [max_credit_months, max_dpd, ever_dpd_count]:\n",
    "    bureau_balance = pd.merge(mini_bureau_balance,pd.DataFrame(feature).reset_index(),on=\"SK_ID_BUREAU\")\n",
    "    bureau_balance.rename(columns={0:feature.name}, inplace=True)\n",
    "    \n",
    "#clean for np.nan\n",
    "sk = bureau_balance[\"SK_ID_BUREAU\"]\n",
    "bureau_balance = pd.get_dummies(bureau_balance,prefix=\"cat\",dummy_na=True)\n",
    "bureau_balance = handle_missing_inf(bureau_balance, True)\n",
    "bureau_balance[\"SK_ID_BUREAU\"] = sk\n",
    "\n",
    "bureau_balance.drop_duplicates(\"SK_ID_BUREAU\",inplace=True)\n",
    "bureau_balance = bureau_balance.loc[:,[\"SK_ID_BUREAU\",\"max_dpd\",\"max_credit_months\"]]\n",
    "#and then put the cleaned file onto bureau iteself\n",
    "# bureau = pd.merge(bureau,bureau_balance,on=\"SK_ID_BUREAU\")\n",
    "\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))\n",
    "\n",
    "mini_bureau_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering (continued)\n",
    "\n",
    "Now, we'll do some additional engineering on the high level information from each loan the people have had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-20 16:24:08.954093\n",
      "Entered original loop for replace_na\n",
      "removing all non-numeric columns\n",
      "complete at 2018-08-20 16:32:46.886252\n",
      "total runtime: 0:08:37.932159\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "grp = bureau.groupby(\"SK_ID_CURR\")\n",
    "\n",
    "#get the max number of months of credit for this loan\n",
    "max_credit_days = grp.apply(lambda x: np.amin(x[\"DAYS_CREDIT\"]))\n",
    "max_credit_days.name = \"max_credit_days\"\n",
    "\n",
    "#get the max value for dpd, which maps to how late the person ever was. 5 is the worst, 0 is the best\n",
    "future_end_credit_day = grp.apply(lambda x: np.amax(x[\"DAYS_CREDIT_ENDDATE\"]))\n",
    "future_end_credit_day.name = \"future_end_credit_day\"\n",
    "\n",
    "#are you dpd on anything at the time of application?\n",
    "curr_dpd = grp.apply(lambda x: np.amax(x[\"CREDIT_DAY_OVERDUE\"]))\n",
    "curr_dpd.name = \"curr_dpd\"\n",
    "\n",
    "#whats your total current credit obligation\n",
    "curr_obligation = grp.apply(lambda x: np.sum(x[\"AMT_CREDIT_SUM_DEBT\"]))\n",
    "curr_obligation.name = \"curr_obligation\"\n",
    "\n",
    "#what's your current total credit limit\n",
    "curr_limit = grp.apply(lambda x: np.sum(x[\"AMT_CREDIT_SUM\"]))\n",
    "curr_limit.name = \"curr_limit\"\n",
    "\n",
    "#what's your current total utilization ratio\n",
    "curr_util_ratio = curr_obligation/curr_limit\n",
    "curr_util_ratio.name = \"curr_util_ratio\"\n",
    "\n",
    "#how many loans have you ever had\n",
    "num_total_loans = grp.apply(lambda x: x[\"SK_ID_CURR\"].nunique())\n",
    "num_total_loans.name = \"num_total_loans\"\n",
    "\n",
    "#how many of those are active\n",
    "active_loans = grp.apply(lambda x: x[x[\"CREDIT_ACTIVE\"]=='Active'][\"SK_ID_CURR\"].nunique())\n",
    "active_loans.name = \"active_loans\"\n",
    "\n",
    "#how much has the person ever prolonged credit\n",
    "tot_prolonged = grp.apply(lambda x: np.sum(x[\"CNT_CREDIT_PROLONG\"]))\n",
    "tot_prolonged.name = \"tot_prolonged\"\n",
    "\n",
    "#put these features in the dataset\n",
    "for feature in [max_credit_days, future_end_credit_day, curr_dpd, curr_obligation, curr_limit, curr_util_ratio, num_total_loans, active_loans ,tot_prolonged]:\n",
    "    bureau = pd.merge(bureau,pd.DataFrame(feature).reset_index(),on=\"SK_ID_CURR\")\n",
    "    bureau.rename(columns={0:feature.name}, inplace=True)\n",
    "\n",
    "sk = bureau[\"SK_ID_CURR\"]\n",
    "\n",
    "bureau = pd.get_dummies(bureau,prefix=\"cat\",dummy_na=True)\n",
    "bureau = handle_missing_inf(bureau, True)\n",
    "bureau[\"SK_ID_CURR\"] = sk\n",
    "\n",
    "bureau.drop_duplicates(\"SK_ID_CURR\",inplace=True)\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean all datasets\n",
    "\n",
    "Now, we'll apply those cleaning functions that we developed above. It will first categorize all string variables in the dataset. Then, it will turn replace all the numerical columns with a unit adjusted column (with np.nans replaced with the mean of the column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-20 16:32:46.906307\n",
      "Entered original loop for replace_na\n",
      "removing all non-numeric columns\n",
      "Entered original loop for replace_na\n",
      "removing all non-numeric columns\n",
      "Entered original loop for replace_na\n",
      "removing all non-numeric columns\n",
      "complete at 2018-08-20 16:34:48.514637\n",
      "total runtime: 0:02:01.609334\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "#clean categorical variables and replace na values\n",
    "\n",
    "train_data = merge_bureau(train_data)\n",
    "train_data = categorize_string_var(train_data)\n",
    "train_data = handle_missing_inf(train_data,True)\n",
    "train_cols = train_data.columns\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "train_data = pd.DataFrame(train_data, columns=train_cols)\n",
    "train_data.to_csv(\"./train_data.csv\")\n",
    "\n",
    "test_data = merge_bureau(test_data)\n",
    "test_data = categorize_string_var(test_data)\n",
    "test_data = handle_missing_inf(test_data,True)\n",
    "test_cols = test_data.columns\n",
    "test_data = scaler.transform(test_data)\n",
    "test_data = pd.DataFrame(test_data, columns=train_cols)\n",
    "test_data.to_csv(\"./test_data.csv\")\n",
    "\n",
    "dev_data = merge_bureau(dev_data)\n",
    "dev_data = categorize_string_var(dev_data)\n",
    "dev_data = handle_missing_inf(dev_data,True)\n",
    "dev_cols = dev_data.columns\n",
    "dev_data = scaler.transform(dev_data)\n",
    "dev_data = pd.DataFrame(dev_data, columns=train_cols)\n",
    "dev_data.to_csv(\"./dev_data.csv\")\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Nueral Net\n",
    "\n",
    "Now, we'll do final prep for the dataset to be fed to the nueral net, which means putting the bureau data on and stripping off the identifier, which is decidedly not a feature in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-21 14:32:11.780157\n",
      "complete at 2018-08-21 14:32:31.295685\n",
      "total runtime: 0:00:19.516030\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "train_data = pd.read_csv(\"./train_data.csv\")\n",
    "test_data = pd.read_csv(\"./test_data.csv\")\n",
    "dev_data = pd.read_csv(\"./dev_data.csv\")\n",
    "\n",
    "# train_data_nn = merge_bureau(train_data)\n",
    "train_data.drop(labels=\"SK_ID_CURR\", axis=1, inplace=True)\n",
    "\n",
    "# test_data_nn = merge_bureau(test_data)\n",
    "test_data.drop(labels=\"SK_ID_CURR\", axis=1, inplace=True)\n",
    "\n",
    "# dev_data_nn = merge_bureau(dev_data)\n",
    "dev_data.drop(labels=\"SK_ID_CURR\", axis=1, inplace=True)\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "lasso = LassoCV(max_iter = 10000, alphas = [x*.1 for x in range(1,100)], random_state = 42, n_jobs = -1)\n",
    "selector = RFE(lasso,15)\n",
    "\n",
    "selector.fit(train_data,train_labels)\n",
    "train_to_model = train_data.iloc[:,selector.support_]\n",
    "test_to_model = test_data.iloc[:,selector.support_]\n",
    "dev_to_model = dev_data.iloc[:,selector.support_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest\n",
    "\n",
    "First, we're going to train a random forest ensemble classifier as a baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-b88001aa80a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "#train and fit random forest regression\n",
    "rf = RandomForestRegressor(n_estimators = 50, max_leaf_nodes = 10, n_jobs = -1)\n",
    "rf.fit(train_to_model, train_labels)\n",
    "test_predict = rf.predict(test_to_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict the results for test_labels\n",
    "test_predict = np.where(test_predict > .5, 1, 0)\n",
    "cfmx = confusion_matrix(test_labels, test_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cfmx,['no-default','default'])\n",
    "\n",
    "print(\"F1 Score is: {}\".format(f1_score(test_labels, test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBM\n",
    "\n",
    "Next, we'll train a gradient boosted tree to improve on the previous resulta and compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train and fit a Gradient Boosted Tree\n",
    "gbt = GradientBoostingClassifier()\n",
    "gbt.fit(train_to_model, train_labels)\n",
    "test_predict = gbt.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict results for trest_predict\n",
    "test_predict = np.where(test_predict > .5, 1, 0)\n",
    "cfmx = confusion_matrix(test_labels, test_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cfmx,['no-default','default'])\n",
    "\n",
    "print(\"F1 Score is: {}\".format(f1_score(test_labels, test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM\n",
    "\n",
    "Next, we'll train a support vector machine with some autotuning to see the accuracy of this alogorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm = SVC()\n",
    "# svm.fit(train_data, train_labels)\n",
    "# test_predict = svm.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_predict = np.where(test_predict > .5, 1, 0)\n",
    "# cfmx = confusion_matrix(test_labels, test_predict)\n",
    "# np.set_printoptions(precision=2)\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cfmx,['no-default','default'])\n",
    "\n",
    "# print(\"F1 Score is: {}\".format(f1_score(test_labels, test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Nueral Net\n",
    "\n",
    "Now, we'll train a nueral net using the prepared data. It will use a sigmoid function so that we can predict probabilities from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began at 2018-08-21 14:33:18.471216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_dim=143, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "230633/230633 [==============================] - 51s 223us/step - loss: 0.3001 - acc: 0.9193\n",
      "Epoch 2/25\n",
      "230633/230633 [==============================] - 50s 216us/step - loss: 0.2841 - acc: 0.9195\n",
      "Epoch 3/25\n",
      "230633/230633 [==============================] - 47s 203us/step - loss: 0.2908 - acc: 0.9195\n",
      "Epoch 4/25\n",
      "230633/230633 [==============================] - 52s 225us/step - loss: 0.3375 - acc: 0.9195\n",
      "Epoch 5/25\n",
      "230633/230633 [==============================] - 49s 210us/step - loss: 0.3996 - acc: 0.9195\n",
      "Epoch 6/25\n",
      "230633/230633 [==============================] - 51s 221us/step - loss: 0.3818 - acc: 0.9195\n",
      "Epoch 7/25\n",
      "230633/230633 [==============================] - 49s 211us/step - loss: 0.3708 - acc: 0.9195\n",
      "Epoch 8/25\n",
      "230633/230633 [==============================] - 48s 210us/step - loss: 0.6484 - acc: 0.9195\n",
      "Epoch 9/25\n",
      "230633/230633 [==============================] - 60s 258us/step - loss: 0.8754 - acc: 0.9195\n",
      "Epoch 10/25\n",
      "230633/230633 [==============================] - 55s 237us/step - loss: 1.2976 - acc: 0.9195\n",
      "Epoch 11/25\n",
      "230633/230633 [==============================] - 55s 236us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 12/25\n",
      "230633/230633 [==============================] - 55s 238us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 13/25\n",
      "230633/230633 [==============================] - 55s 237us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 14/25\n",
      "230633/230633 [==============================] - 56s 241us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 15/25\n",
      "230633/230633 [==============================] - 56s 242us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 16/25\n",
      "230633/230633 [==============================] - 56s 245us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 17/25\n",
      "230633/230633 [==============================] - 55s 237us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 18/25\n",
      "230633/230633 [==============================] - 55s 237us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 19/25\n",
      "230633/230633 [==============================] - 56s 243us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 20/25\n",
      "230633/230633 [==============================] - 55s 238us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 21/25\n",
      "230633/230633 [==============================] - 54s 236us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 22/25\n",
      "230633/230633 [==============================] - 55s 238us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 23/25\n",
      "230633/230633 [==============================] - 55s 237us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 24/25\n",
      "230633/230633 [==============================] - 55s 239us/step - loss: 1.2975 - acc: 0.9195\n",
      "Epoch 25/25\n",
      "230633/230633 [==============================] - 55s 240us/step - loss: 1.2975 - acc: 0.9195\n",
      "complete at 2018-08-21 14:55:36.019068\n",
      "total runtime: 0:22:17.547852\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"began at {}\".format(datetime.datetime.now()))\n",
    "\n",
    "#bring in required packages\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Create a 20 neuron hidden layer with Linear Rectified activation function.\n",
    "model.add(Dense(20, input_dim=train_data.shape[1], init='uniform', activation='relu'))\n",
    "\n",
    "# Create a 8 neuron hidden layer.\n",
    "model.add(Dense(20, init='uniform', activation='relu'))\n",
    "\n",
    "# Adding a output layer with sigmoid activation\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model on teh train data\n",
    "model.fit(train_data, train_labels, epochs=25, batch_size=5)\n",
    "\n",
    "print(\"complete at {}\".format(datetime.datetime.now()))\n",
    "print(\"total runtime: {}\".format(datetime.datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f11c9fcd8622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdev_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_to_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcfmx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "#get the predicted classes on the test data\n",
    "test_predict = model.predict(test_to_model)\n",
    "dev_predict = model.predict(dev_to_model)\n",
    "\n",
    "cfmx = confusion_matrix(test_labels, test_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cfmx,['no-default','default'])\n",
    "\n",
    "print(\"F1 Score is: {}\".format(f1_score(test_labels, test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./application_test.csv\")\n",
    "submission = pd.DataFrame(submission.loc[:,\"SK_ID_CURR\"])\n",
    "dev_predict = model.predict(dev_data)\n",
    "predict = pd.DataFrame(dev_predict, columns = [\"TARGET\"])\n",
    "submission[\"TARGET\"] = predict[\"TARGET\"]\n",
    "\n",
    "submission.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
