{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lxml.html as lh\n",
    "from lxml.html import parse\n",
    "import urllib.request, json\n",
    "from urllib.request import urlopen\n",
    "from lxml import objectify\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, zipfile, io\n",
    "import html5lib\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_standardize(df):\n",
    "    #function to standardize columns upon readin\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.lstrip() for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.rstrip() for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.replace(' ', '') for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.replace('#', 'num') for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.replace('/', '_') for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.replace(',', '') for x in df.columns])), inplace=True)\n",
    "    df.rename(columns=dict(zip([x for x in df.columns],[x.replace('.', '') for x in df.columns])), inplace=True)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def drop_duplicate_columns(df):\n",
    "    #function to identify and then drop duplicated column names\n",
    "    \n",
    "    #get col list\n",
    "    Cols = list(df.columns)\n",
    "    \n",
    "    #start looping through\n",
    "    for i,item in enumerate(df.columns):\n",
    "        #change the name of the column to \"toDROP\" if its already occured\n",
    "        if item in df.columns[:i]: Cols[i] = \"toDROP\"\n",
    "    df.columns = Cols\n",
    "    #drop the identified duplicates\n",
    "    df = df.drop(\"toDROP\",1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dcroadrunners.org/races/race-results/2017-results/3160-2017-age-handicapped-4-miler.html\n",
      "https://www.athlinks.com/event/20899/results/Event/603815/Results\n",
      "https://www.athlinks.com/event/33857/results/Event/612117/Results\n",
      "http://racepacket.com/rsltwrap1.php?id=7721\n",
      "https://www.athlinks.com/Master/138877/Event/609930/Results\n",
      "http://www.runrocknroll.com/finisher-zone/search-and-results/?eventid=42\n",
      "http://www.racepacket.com/tidalbasin/20170215.html\n",
      "https://www.athlinks.com/Master/13536/Event/586956/Course/882049/Results\n",
      "https://results.chronotrack.com/event/results/event/event-22712\n",
      "http://www.athletic.net/CrossCountry/Results/Meet.aspx?Meet=116060\n",
      "http://results.xacte.com/?mid=613\n",
      "https://results.chronotrack.com/event/results/event/event-25231\n",
      "https://www.athletic.net/CrossCountry/Results/Meet.aspx?Meet=118783\n",
      "https://results.chronotrack.com/event/results/event/event-25075\n",
      "https://results.chronotrack.com/event/results/event/event-19723\n",
      "https://s3.amazonaws.com/media.racebx.com/transfer/gen/5/7/9/579e073d-3e2c-4730-9979-65b45206f5f9/MMFFX-12-14.htm\n",
      "https://results.chronotrack.com/event/results/event/event-20983\n",
      "http://www.safetyandhealthfoundation.org/20160628.html\n",
      "http://www.zippyraceresults.com/searchrwt.php?ID=5851 has type error\n",
      "396\n",
      "http://www.racepacket.com/rsltwrap1.php?id=7475\n",
      "http://s3.amazonaws.com/media.racebx.com/transfer/gen/5/6/f/56ffcdb8-388c-4f19-baf6-78ea5206f5f9/reston-mm-1214.htm\n",
      "http://www.areep.com/events/usatf12k/2015/\n",
      "http://runwashingtontiming.com/results/2015/1114mddc.txt\n",
      "http://www.athletic.net/CrossCountry/Results/Meet.aspx?Meet=105735#Results\n",
      "http://www.runhigh.com/2015RESULTS/R101715DA.html\n",
      "http://www.mdtiming.com/2015/navymile/index.html\n",
      "http://dcxc15.s3-website-us-east-1.amazonaws.com/#/\n",
      "http://s3.amazonaws.com/media.racebx.com/transfer/gen/5/5/f/55ff5bc3-6e04-4820-842c-110ac0a86526/2015-Mighy-Mile-Leesburg10-11-Mighty-Mile-12-14.html\n",
      "http://s3.amazonaws.com/media.racebx.com/transfer/txt/5/5/b/55b4e51c-d9a0-45ac-9a90-371bc0a86526.txt\n",
      "http://www.dcroadrunners.org/races/race-results/2015-results/2693-2015-dcrr-track-championships-mile-heats.html\n",
      "http://www.zippyraceresults.com/searchrwt.php?ID=4019 has no tables\n",
      "http://www.zippyraceresults.com/searchrwt.php?ID=3330\n",
      "1801\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(7568960 bytes read, 8517765 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-56ce12287eca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"zippyraceresults\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mrace_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, tupleize_cols, thousands, encoding)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[0m_validate_header_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m     return _parse(flavor, io, match, header, index_col, skiprows,\n\u001b[1;32m--> 874\u001b[1;33m                   parse_dates, tupleize_cols, thousands, attrs, encoding)\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\html.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, header, index_col, skiprows, parse_dates, tupleize_cols, thousands, attrs, encoding)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kredfield\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[1;34m(exc, traceback)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(7568960 bytes read, 8517765 more expected)"
     ]
    }
   ],
   "source": [
    "url = \"http://www.areep.com/rwt/results/results_master.php\"\n",
    "\n",
    "#open up the url and turn it into an html response element\n",
    "read_text = urlopen(url).read()\n",
    "\n",
    "#extract all tables from the html\n",
    "table = lh.fromstring(read_text)\n",
    "\n",
    "# extract the text from `<td>` tags\n",
    "data = [[elt.text_content() for elt in tr.xpath('td')] for tr in table.xpath('//tr')]\n",
    "\n",
    "#and turn the text into dataframes \n",
    "#use the first row of the table as the header\n",
    "df = pd.DataFrame(data[2:], columns=data[1])\n",
    "\n",
    "#make a blank dataframe with the columns you want to fill for appending later\n",
    "dfcolumns = ['place', 'bib', 'name', 'age', 'sex', 'div', 'city', 'time', 'nettime','pace', 'total_participants']\n",
    "race_results = pd.DataFrame(columns = dfcolumns)\n",
    "big_sheet = pd.DataFrame(columns = dfcolumns)\n",
    "\n",
    "# extract the href attribute values into a new column\n",
    "df['refname'] = table.xpath('//tr/td/a/@href')\n",
    "\n",
    "#start looking into the compiled dataframe for specific race results\n",
    "for i, row in df.iterrows():\n",
    "    url = df.ix[i,'refname']\n",
    "    \n",
    "    #a little bit of user control here in case the program craps out and you need to restart it from a point further on\n",
    "    #set threshold to 0 if you want to start from the beginning\n",
    "    if i > 0:\n",
    "        \n",
    "        #anything that isn't in the format of \"zippyraceresults.com\" will return errors, so we'll just look at these\n",
    "        if \"zippyraceresults\" in url:\n",
    "            try:\n",
    "                race_table = pd.read_html(url, parse_dates = True)\n",
    "                \n",
    "            except ValueError:\n",
    "                print(url, \"has no tables\")\n",
    "                continue\n",
    "            except TypeError:\n",
    "                print(url, \"has type error\")\n",
    "                print(i)\n",
    "                continue\n",
    "            except:\n",
    "                print(url)\n",
    "                print(i)\n",
    "                raise\n",
    "            \n",
    "            #the way the table is returned, you only need every other row\n",
    "            #so only take the first out of every set of two rows\n",
    "            race_table = race_table[0].iloc[::2].reset_index(drop=True)\n",
    "            \n",
    "            #standardize the variable names using the function above\n",
    "            race_table = variable_standardize(race_table)\n",
    "            \n",
    "            #test if there are duplicate columns and, if there are, drop the duplicates\n",
    "            if len(race_table.columns) != len(set(race_table.columns)):\n",
    "                race_table = drop_duplicate_columns(race_table)\n",
    "            \n",
    "            #count up the total number of people running in the race\n",
    "            race_table['total_participants'] = race_table.count()[0]\n",
    "\n",
    "            #sometimes, the table won't include the place\n",
    "            #if that's true, we'll take matters into our own hands\n",
    "            if \"place\" not in race_table.columns:\n",
    "                    race_table.sort_values(\"time\", inplace=True)\n",
    "                    race_table[\"place\"] = race_table.rank()\n",
    "                    \n",
    "            #only keep the top 10\n",
    "            try:\n",
    "                race_table = race_table[race_table[\"place\"]<=50]\n",
    "                race_table = race_table.dropna(axis=1, how=\"all\")\n",
    "\n",
    "            except KeyError:\n",
    "                print(url, \"is missing place still\")\n",
    "                race_table.sort_values(\"time\")\n",
    "                continue\n",
    "            except TypeError:\n",
    "                print(url, \"has type error\")\n",
    "                print(i)\n",
    "                continue\n",
    "            except:\n",
    "                print(url, \"other error type\")\n",
    "                print(i)\n",
    "                raise\n",
    "        else: \n",
    "            print(url)\n",
    "            continue\n",
    "            \n",
    "        #in your returned result to the big sheet, capture the name of the race and the date\n",
    "        race_table[\"race_name\"] = row.Name\n",
    "        race_table['date'] = row.Date\n",
    "        \n",
    "        #put this all together to get a full record of the race and the top 10\n",
    "        race_results = pd.concat([race_table,race_results],ignore_index=True)\n",
    "        \n",
    "        #and stick it on the running tab of results\n",
    "        big_sheet = pd.concat([big_sheet,race_results],ignore_index=True)\n",
    "        \n",
    "        #clear the definition for race_results\n",
    "        del race_results\n",
    "        race_results = pd.DataFrame(columns = dfcolumns)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#work off a copied version\n",
    "all_races = big_sheet.copy()\n",
    "\n",
    "#Combine names if they are separated into first and last\n",
    "all_races.name = np.where(all_races.name.isnull(),all_races.firstname + \" \" + all_races.lastname, all_races.name)\n",
    "#if the field \"time\" is populated instead of \"nettime\", fill over\n",
    "all_races.nettime = np.where(all_races.nettime.isnull(),all_races.time, all_races.nettime)\n",
    "\n",
    "#fill state with last two chars of city if blank\n",
    "all_races[\"StateClean\"] = np.where(all_races.state.isnull(),all_races.city.str[(all_races.city.str.len()-2):],all_races.state)\n",
    "\n",
    "#uppercase state\n",
    "all_races[\"StateClean\"] = all_races.StateClean.str.upper()\n",
    "\n",
    "\n",
    "#delete last two char of state if city is blank\n",
    "all_races[\"CityClean\"] = np.where(all_races.state.isnull(),all_races.city.str[:(all_races.city.str.len()-3)],all_races.city)\n",
    "#propercase city\n",
    "all_races.CityClean = all_races.CityClean.str.title()\n",
    "\n",
    "\n",
    "\n",
    "#Set up the columns you want to keep in the raw file\n",
    "keep_cols = ['age','date','CityClean','name','nettime','pace','place','race_name','sex','StateClean','total_participants']\n",
    "drop_list = []\n",
    "\n",
    "for col in all_races.columns:\n",
    "    if col not in keep_cols:\n",
    "        #put columns that you don't want into a list to drop later\n",
    "        drop_list.append(col)\n",
    "    else:\n",
    "        \n",
    "        #but, if you want to keep them, strip leading and trailing\n",
    "        if isinstance(all_races.ix[1,col],str):\n",
    "            all_races[col] = all_races[col].str.strip()\n",
    "\n",
    "#drop those columns you put in a list to drop\n",
    "all_races.drop(drop_list, axis=1, inplace=True)\n",
    "\n",
    "#set up all the appropriate types of races in the file\n",
    "race_types = ['5K','10K','8K','10 Miler','Marathon','Half Marathon','3.5 Miler','20K','Four Miler', \"15K\", '9K', '5 Miler', 'Metric Marathon' ,'50 Miler', '9.2K', '3 Miler', '4.5 Miler', '3K', '5000m', \"Pi Miler\", \"1 Miler\", \"2 Miler\", \"4 Miler\", \"50K\"]\n",
    "\n",
    "#and apply them in a field called race_type\n",
    "all_races['race_type'] = all_races.race_name.apply(lambda x: process.extractOne(x, race_types)[0] if process.extractOne(x, race_types)[1] >= 60 else \"No Match\")\n",
    "check_type = all_races[all_races.race_type == \"No Match\"]\n",
    "\n",
    "\n",
    "\n",
    "#send cleaned file to excel\n",
    "\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\kredfield\\Documents\\Washington Run Times\\All_Races.xlsx')\n",
    "big_sheet.to_excel(writer, index=False, sheet_name=\"raw\")\n",
    "all_races.to_excel(writer, index=False, sheet_name=\"Top50\")\n",
    "writer.save()\n",
    "\n",
    "all_races"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
